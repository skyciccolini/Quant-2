---
title: "Problem Set 4"
author:
  - Skylar Ciccolini
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    toc-title: Contents
---

```{r}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)
library(brms)
library(priorsense)
library(tidybayes)
library(distributional)
library(modelr)
library(bayesplot)
library(rethinking)

options(brms.backend = "cmdstan")
theme_set(theme_cowplot(font_size = 10))

# Required files for this problem set:
#   - Earwigs.csv
#   - Heart_Transplants.csv
#   - Bird_Plasma.xlsx
#   - Milk.xlsx

# Install updated brms from github
#   remotes::install_github("paul-buerkner/brms")
```

In this problem set, before we get to the models, we want give some demonstration code about two packages that help with some aspects of Bayesian inference. We'll walk through code so you have templates to use for later analyses in this problem set.

## `priorsense` Package

Usually, those new to Bayesian analysis have many questions about how to set a prior and how to evaluate whether the chosen prior is appropriate. We will show you a new package that helps to assess this second question - is my prior appropriate? This package is the `priorsense` package[^1]. If you are interested is a less technical introduction, here is a short [video describing the package](https://www.youtube.com/watch?v=TBXD3HjcIps). We decided to leave this out of the lectures in part because it would have taken time away from other topics and because it is an area of still active research.

[^1]: Kallioinen, N., T. Paananen, P.-C. BÃ¼rkner, and A. Vehtari. 2024. Detecting and diagnosing prior and likelihood sensitivity with power-scaling. Stat. Comput. 34.

The approach that Kallioinen et al. take is to use importance sampling (as in PSIS-LOO-CV) of the prior or likelihood raised to exponent (the "power" in power-scaling) to detect instances of *prior-data conflict* wherein the prior contains too much information (e.g., is too constrained) or the likelihood (data) has too little information, or some combination of the two.

The approach is simply to give either the prior or the likelihood more (or less) power by raising it to an exponent $\alpha$ that varies around 1 (i.e., no scaling). For example, in testing the prior $Pr(\theta)$ sensitivity:

$$Pr(\theta|y) \sim Pr(y|\theta) Pr(\theta)^\alpha$$

the prior is raised to an exponent $\alpha$ that can vary. The response of the posterior $Pr(\theta|y)$ to changing the strength of the prior tells us how sensitive the model is to the prior. Note here that we are only looking at the numerator of Bayes' Rule, because MCMC methods make dealing with the probability of the data $Pr(y)$ unnecessary.

The prior scaling approach is a complementary to prior predictive simulation that we have been using so far. The general approach would be to develop priors via prior predictive simulation (using the different options in `ulam()` or `brm()` to sample from the prior only) and then check that those priors are adequate using the functions in `priorsense`.

The main function of this package are:

-   `powerscale_sequence()` evaluates the prior/likelihood sensitivity across a range of powers. This function can be wrapped in either `powerscale_plot_dens()` or `powerscale_plot_quantity()` to plot changes in the posterior densities or the dependency of the posterior on prior or likelihood scaling, respectively.
-   `powerscale_sensitivity()` is the main function to test the sensitivity of the prior and likelihood via power-scaling.

Install `priorsense` from CRAN.

We will use the Earwigs data from Problem Set 3 to explore how to use this package.

Load the data and plot:

```{r}
# Change the path to the location of Earwigs.csv on your computer
EW <- read_csv("raw_data/Earwigs.csv", show_col_types = FALSE)

ggplot() +
  geom_point(
    data = EW,
    aes(Density, Proportion_forceps),
    color = "steelblue",
    size = 3
  )
```

Check the variables that can have priors in the `brm()` model:

```{r}
get_prior(Proportion_forceps ~ 1 + Density, data = EW)
```

We will skip the prior predictive simulation in this example. To use the `priorsense` functions, you have to fit the model with the data (i.e., not sampling from the prior only). So in practice you would do the prior predictive simulation here, sampling from the prior to get a prospective set of priors. Then you would use the functions in `priorsense` to evaluate the priors with the model.

For now, we will set the priors to be very bad (which we know from doing Problem Set 3), to see what the diagnostics look like:

```{r}
fm <- brm(
  Proportion_forceps ~ 1 + Density,
  data = EW,
  prior = c(
    prior(normal(0, 0.0001), class = b),
    prior(normal(0, 0.0001), class = Intercept),
    prior(normal(0, 0.01), class = sigma)
  ),
  refresh = 0
)
```

### Power-scale sensitivity visual diagnostics

`powerscale_plot_dens()` plots overlapping density plots color coded by the range of \`$\alpha$ power-scaling exponents in the tested range. When the lines overlap, it indicates that the density estimate is not sensitive to power-scaling. Considering the prior, for example, if the lines differ, then it means that the prior density changes when scaled by a power (bad). So what we want to see is that the density lines for the prior are superimposed. For the likelihood, the lines should not overlap, indicating that the prior and the data are able to change the likelihood.

Because the scales of the variables are so different, we will plot them separately. Notice the embedded `powerscale_sequence(fm)`. We could pre-compute this and pass to the function just as well (if there were more data would be advantageous for speed).

```{r}
powerscale_plot_dens(
  powerscale_sequence(fm),
  quantity = c("mean", "sd"),
  variable = "b_Density"
)

powerscale_plot_dens(
  powerscale_sequence(fm),
  quantity = c("mean", "sd"),
  variable = "b_Intercept"
)

powerscale_plot_dens(
  powerscale_sequence(fm),
  quantity = c("mean", "sd"),
  variable = "sigma"
)
```

You can see how the all of the priors are sensitive to scaling, particularly `sigma`. We also get messages about high Pareto $k$ value, indicating poor fit.

`powerscale_plot_quantity()` visualizes the rate of change in the posterior as $\alpha$ changes. Ideally we would like to see a flat-ish line for the prior, indicating that the prior is not sensitive to scaling. There are many options for the divergence measure, but the default "Cumulative Jensen-Shannon distance" (`cjs_dist`) seems to work fine.

```{r}
powerscale_plot_quantities(
  powerscale_sequence(fm),
  quantity = c("mean", "sd"),
  variable = c("b_Density", "b_Intercept", "sigma")
)
```

## Power-scale sensitivity table

Finally, the function`powerscale_sensitivity()` makes a table of sensitivity values. Values \>0.05 indicates sensitivity of the prior or likelihood. The last column provides a diagnosis.

```{r}
powerscale_sensitivity(fm)
```

Here we have a "weak likelihood" for the first two rows, because the priors on the means and Intercept are way too strong (Normal(0, 0.0001)). This means that the data are insufficient to move the likelihood away from the prior.

The prior for `sigma` is also poor, resulting in a prior-data conflict, where one goes up and one goes down as $\alpha$ changes

### Improved priors

Let's use the priors that we developed for problem set 3 and hopefully see a better pattern.

```{r}
fm <- brm(
  Proportion_forceps ~ 1 + Density,
  data = EW,
  prior = c(
    prior(normal(0, 0.1), class = b),
    prior(normal(0, 1), class = Intercept),
    prior(normal(0, 1), class = sigma)
  ),
  refresh = 0,
  iter = 5e3
)
```

Evaluating the priors:

```{r}
powerscale_plot_dens(
  powerscale_sequence(fm),
  quantity = c("mean", "sd"),
  variable = "b_Density"
)

powerscale_plot_dens(
  powerscale_sequence(fm),
  quantity = c("mean", "sd"),
  variable = "b_Intercept"
)

powerscale_plot_dens(
  powerscale_sequence(fm),
  quantity = c("mean", "sd"),
  variable = "sigma"
)

powerscale_plot_quantities(
  powerscale_sequence(fm),
  quantity = c("mean", "sd"),
  variable = c("b_Density", "b_Intercept", "sigma")
)

powerscale_sensitivity(fm)
```

Notice how in the density plots, densities of the priors are all overlapping (lack of sensitivity) and the posteriors are not overlapping (the data is able to inform the posterior). The quantities plot shows relatively flat lines for the priors and likelihoods that are sensitive to scaling. Finally the table has all values \< 0.05 for the prior.

## `tidybayes` Package

We want to add one more set of analysis tools to our general Bayesian inference kit: the `tidybayes` package. `tidybayes` has a variety of functions for extracting parts of fit models (from lots of model fitting interfaces), augmenting model fits with various kinds of predicted values, and making some very impressive visualizations.

The documentation has a \[page of visualizations from `brms` models\]http://mjskay.github.io/tidybayes/articles/tidy-brms.html().

`tidybayes` is particularly useful for working with the posteriors of multilevel models, which is what the demo code that the documentation provides is based on. Our usage here will be a little more pedestrian, but we can still see how useful the package can be.

We will adapt some of the demo code to plot the posterior for the earwigs model we just fit.

When you are trying to figure out what the variable names are in a model, the function `get_variabless()` returns them:

```{r}
library(tidybayes)

get_variables(fm)
```

By default, `brm()` returns parameters prepended with `b_` for "b" parameters (what are often called main or fixed effects) and `r_` for random/multilevel effects (though we aren't doing multilevel models in this module, it's useful to know).

`tidy_draws()` is the simplest way to extract a posterior. You can see how it returns a lot of diagnostics as well: acceptance statistic, tree depth, step size, and whether that draw was a divergence or not.

`tidybayes` has a `summary()`-like function `summarise_draws()` (Commonwealth spelling only). We can pipe the output of `tidy_draws()` directly to it.

```{r}
fm |> tidy_draws()

fm |> tidy_draws() |> summarise_draws()
```

Many of the `tidybayes` functions require the posterior to be in a slightly different format, that of an `rvar`. An `rvar` is a compact way to store a distribution of values. We can use `spread_rvars()` to extract only a few of the columns. We then pipe that output to `median_hdi()` to get the median 89% HDI of the posterior for each.

```{r}
fm |>
  spread_rvars(b_Intercept, b_Density, sigma)

fm |>
  spread_rvars(b_Intercept, b_Density, sigma) |>
  median_hdi(.width = 0.89)
```

There are many options in `tidybayes` to plot distributions and intervals. Here is a point + interval plot of the three main parameters.

```{r}
fm |>
  spread_rvars(b_Intercept, b_Density, sigma) |>
  pivot_longer(cols = everything()) |>
  ggplot(aes(y = name, dist = value)) +
  stat_pointinterval(.width = c(0.89, 0.97))
```

`b_Density` is very small relative to the other parameters, so it's variation looks really small in comparison. We might just plot it separately:

```{r}
fm |>
  spread_rvars(b_Density) |>
  pivot_longer(cols = everything()) |>
  ggplot(aes(y = name, dist = value)) +
  stat_pointinterval(.width = c(0.89, 0.97))

```

From this plot you can see how the posterior is credibly different from 0, even though the parameter estimate is small.

`tidybayes` works well with `data_grid()` from the `modelr` package. Like `crossing()` that we have used before, `data_grid()` generates the pairwise combinations of variables that are passed to it, but without needing to include as many details (it will by default use the range of continuous variables and all the levels of factors).

If we then pipe that out to `add_epred_draws()` called with the fitted model, we can create a tibble with the values of `Density` across a range paired with the expected value of `Proportion_forceps`.

The second block of code pipes these values to `ggplot()` to make a plot of the observed data along with ribbons representing the 50%, 89%, and 97% HDIs for the expected values. Remember that these values do not include the standard deviation, so they are relatively narrow.

```{r}
library(modelr)

# Expected parameter estimates

EW |>
  data_grid(Density = seq_range(Density, n = 200)) |>
  add_epred_draws(fm)

EW |>
  data_grid(Density = seq_range(Density, n = 200)) |>
  add_epred_draws(fm) |>
  ggplot(aes(x = Density, y = Proportion_forceps)) +
  stat_lineribbon(aes(y = .epred), .width = c(0.5, 0.89, 0.97), alpha = 0.5) +
  geom_point(data = EW)
```

We can do the same but generate a posterior predictive distribution plot by calling `add_predicted_draws()` instead (note that the variable is `.prediction` rather than `.epred`).

```{r}
# Posterior predictive distribution

EW |>
  data_grid(Density = seq_range(Density, n = 200)) |>
  add_predicted_draws(fm) |>
  ggplot(aes(x = Density, y = Proportion_forceps)) +
  stat_lineribbon(
    aes(y = .prediction),
    .width = c(0.5, 0.89, 0.97),
    alpha = 0.5
  ) +
  geom_point(data = EW)
```

Almost all of the points fall within the 97% interval, just like we would predict. Observe that the lines and edges are pretty rough. We could sample more iterations to smooth those out.

In the analyses below, try to add the packages above to your now pretty well-developed Bayesian modeling routines. Also see if you can work with the `mcmc_` functions from `bayesplot` and `pp_check()` for plotting prior/posterior predictive checks.

These are three models that you saw in Quantitative Methods 1. We will leave much of the details of the analysis to you, providing some guidance for three challenging kinds of models to fit and interpret.

## ANOVA-like

The data in `Heart_Transplants.csv` has data on the `Survival` time (in days) for heart transplant patients with varying degrees of `Mismatch` between the donor and recipient. You will need to convert `Mismatch` to a factor and get the factor in the correct order: low, medium, high. Low indicates a relatively good match and high a poor match. The data have a pronounced right skew.

Load the data, visualize, and transform how you see fit.

```{r}
heart <- read_csv("raw_data/Heart_Transplants.csv", show_col_types = FALSE) |> 
  mutate(Mismatch = factor(Mismatch,
                           levels = c("Low", "Medium", "High")))


ggplot() +
  geom_point(data = heart, aes(x = Mismatch, y = Survival, color = Mismatch),
             position = position_jitter(width = 0.1, seed = 34879),
             size = 3) +
  scale_color_manual(values = c("tomato", "steelblue", "darkorchid")) +
  labs(y = "Survival") +
  theme(legend.position = "none")

```

### Model specification

```{=tex}
\begin{align}
  \mathrm{logSurvival} & \sim Normal(\mu, \sigma) \\
  \mu & = b[\mathrm{Mismatch}] \\
\end{align}
```
### Prior specification and prior predictive check

```{r}
PP <- ulam(
  alist(
    Survival ~ dnorm(mu, sigma),
    mu <- b[Mismatch],
    b[Mismatch] ~ dnorm(600, 5),
    sigma ~ dhalfnorm(5, 5)
  ),
  data = heart,
  sample_prior = TRUE
)

pp_dist <- extract.samples(PP)$b |> 
  as.data.frame() |> 
  rename(low = V1,
         medium = V2,
         high = V3)

pp_dist |> 
  pivot_longer(cols = everything(),
               names_to = "Mismatch",
               values_to = "Survival") |> 
  ggplot(aes(Survival, fill = Mismatch)) +
  geom_histogram(bins = 30) +
  scale_fill_manual(values = c("tomato", "steelblue", "darkorchid")) +
  facet_grid(Mismatch ~ .) +
  cowplot::theme_cowplot() +
  labs(x = "Survival", y = "Count") +
  theme(legend.position = "none")
```

### Final model specification

```{=tex}
\begin{align}
  \mathrm{logSurvival} & \sim Normal(\mu, \sigma) \\
  \mu & = b[\mathrm{Mismatch}] \\
  b[\mathrm{Mismatch}] & \sim Normal(600, 5) \\
  \sigma &\sim HalfNormal(5, 5)\\
\end{align}
```
### Sampling

```{r}
fm <- ulam(
  alist(
    Survival ~ dnorm(mu, sigma),
    mu <- b[Mismatch],
    b[Mismatch] ~ dnorm(600, 5),
    sigma ~ dhalfnorm(5, 5)
  ),
  data = heart,
  sample_prior = FALSE,
  chains = 4,
  iter = 5e3,
  refresh = 0
)
```

### Diagnostics

```{r}
summary(fm)
```

```{r}
traceplot(fm)
trankplot(fm)
```


### Posterior predictive simulation

```{r}
post <- extract.samples(fm)$b |> 
  as.data.frame() |> 
  rename(Low = V1,
         Medium = V2,
         High = V3)

post |> 
  pivot_longer(cols = everything(),
               names_to = "Mismatch",
               values_to = "Survival") |> 
  ggplot(aes(Survival, color = Mismatch)) +
  geom_density(linewidth = 0.5) +
  geom_point(data = heart, aes(x = Survival, y = 0, color = Mismatch),
             size = 3) +
  scale_color_manual(values = c("tomato", "steelblue", "darkorchid")) +
  facet_grid(Mismatch ~ .) +
  cowplot::theme_cowplot() +
  labs(x = "Survival", y = "Count") +
  theme(legend.position = "none")
```

### Summarizing the posterior

```{r}
###



```

Test the hypothesis that Medium and High mismatch differ from Low using contrasts.

```{r}
post_contrast <- post |> 
  mutate(`Medium vs. Low` = Medium - Low,
         `High vs. Low` = Medium - Low,
         .keep = "none")

post_contrast |> 
  pivot_longer(cols = everything(),
               names_to = "Contrast", values_to = "Difference") |> 
  ggplot(aes(Difference, color = Contrast)) +
  scale_color_manual(values = c("steelblue", "darkorchid")) +
  geom_density(linewidth = 1.5)

apply(post_contrast, MARGIN = 2, FUN = HPDI) ### this seems wrong to me, why HPDI exactly the same?
```

## 2x2 factorial design

The file `Bird_Plasma.xlsx` contains factorial data on blood plasma calcium concentration (`Calcium`, in mg Ca per 100 mL plasma) in male and female birds (`Sex`) each of which was treated or not with a hormone (`Treatment`).

-   Load the data, and convert hormone and sex to factors.
-   The levels of `Treatment` are "Hormone" and "None". Relevel `Treatment` so that "None" is the base level.
-   Plot a reaction norm of Calcium vs. Sex, with color encoding Treatment to get a sense for the pattern.

```{r}
#| message: false

bird <- readxl::read_xlsx("raw_data/Bird_Plasma.xlsx") |> 
  mutate(Treatment = factor(Treatment,
                           levels = c("None", "Hormone")),
         Sex = factor(Sex))

ggplot(bird, aes(x = Sex, y = Calcium, group = Treatment, color = Treatment)) +
  geom_point() +
  geom_line() ### only one line, not multiple

```

### Model specification

We have a factorial model, so we would like to model the two main effects: Sex and Treatment as well as the Sex by Treatment interaction term. Interactions between categorical variables are complicated to code in Bayesian models. Although you can just input the model like you would with `lm()`: `Sex * Treatment`, specifying the priors might be tricky and getting the posteriors sorted out as well.

One approach that works well in some (most? all?) situations is to create a new composite variable that combines the two other variables. Thus the four factorial groups become a single factor with four levels (Female-Hormone, Female-None, Male-Hormone, and Male-None). Because we are testing hypotheses using contrasts (subtracting posterior distributions), we don't have to worry about the usual main effects and interaction *P*-value based hypothesis tests.

You can do this with a simple mutate, joining the two variables:

```{r}
#| eval: false

bird <- bird |>
  mutate(Sex_Trt = factor(paste(Sex, Treatment, sep = "_")))
```

One additional advantage of this approach is that you only need to specify a single prior for all the groups.

```{=tex}
\begin{align}
  \mathrm{Calcium} & \sim Normal(\mu, \sigma) \\
  \mu & = b[\mathrm{Sex\_Trt}] \\
\end{align}
```
### Prior specification and prior predictive check

There are only 5 points per group, so the prior is potentially very powerful relative to the likelihood.

```{r}
PP <- ulam(
  alist(
    Calcium ~ dnorm(mu, sigma),
    mu <- b[Sex_Trt],
    b[Sex_Trt] ~ dnorm(20, 5),
    sigma ~ dhalfnorm(2, 2)
  ),
  data = bird,
  sample_prior = TRUE
)

pp_dist <- extract.samples(PP)$b |> 
  as.data.frame() |> 
  rename(Female_Hormone = V1,
         Female_None = V2,
         Male_Hormone = V3,
         Male_None = V4)

pp_dist |> 
  pivot_longer(cols = everything(),
               names_to = "Treatment",
               values_to = "Calcium") |> 
  ggplot(aes(Calcium, fill = Treatment)) +
  geom_histogram(bins = 30) +
  scale_fill_manual(values = c("tomato", "steelblue", "darkorchid", "forestgreen")) +
  facet_grid(Treatment ~ .) +
  cowplot::theme_cowplot() +
  labs(x = "Calcium", y = "Count") +
  theme(legend.position = "none")
```

### Final model specification

```{=tex}
\begin{align}
  \mathrm{Calcium} & \sim Normal(\mu, \sigma) \\
  \mu & = b[\mathrm{Sex\_Trt}] \\
  b & \sim Normal(20 ,5)\\
  \sigma & \sim HalfNormal(2, 2)\\
\end{align}
```
### Sampling

```{r}
fm <- ulam(
  alist(
    Calcium ~ dnorm(mu, sigma),
    mu <- b[Sex_Trt],
    b[Sex_Trt] ~ dnorm(20, 5),
    sigma ~ dhalfnorm(2, 2)
  ),
  data = bird,
  sample_prior = FALSE,
  chains = 4,
  iter = 5e3,
  refresh = 0
)
```

### Diagnostics

```{r}
summary(fm)
```

```{r}
traceplot(fm)
trankplot(fm)
```

### Posterior predictive simulation

```{r}
post <- extract.samples(PP)$b |> 
  as.data.frame() |> 
  rename(Female_Hormone = V1,
         Female_None = V2,
         Male_Hormone = V3,
         Male_None = V4)

post |> 
  pivot_longer(cols = everything(),
               names_to = "Treatment",
               values_to = "Calcium") |> 
  ggplot(aes(Calcium, fill = Treatment)) +
  geom_histogram(bins = 30) +
  scale_fill_manual(values = c("tomato", "steelblue", "darkorchid", "forestgreen")) +
  facet_grid(Treatment ~ .) +
  cowplot::theme_cowplot() +
  labs(x = "Calcium", y = "Count") +
  theme(legend.position = "none")
```

### Summarizing the posterior

Compare the means of Hormone vs. Control separately by sex.

```{r}
post_contrast <- post |> 
  mutate(`Female Hormone vs. Control` = 
           Female_Hormone - Female_None,
         `Male Hormone vs. Control` = 
           Male_Hormone - Male_None,
         .keep = "none")

post_contrast |> 
  pivot_longer(cols = everything(),
               names_to = "Contrast", values_to = "Difference") |> 
  ggplot(aes(Difference, color = Contrast)) +
  scale_color_manual(values = c("steelblue", "darkorchid")) +
  geom_density(linewidth = 1.5)

apply(post_contrast, MARGIN = 2, FUN = HPDI)

```

## Multiple continuous predictors

Working with multiple continuous predictors also poses some unique challenges (not to mention continuous predictors with interactions). Visualization in particular is not straightforward, because, unless you want a 3D plot, you can't plot 3 continuous variables (1 outcome + 2 predictors) simultaneously. Options include making separate plots, coloring by one predictor by the other, or choosing specific values at which to visualize the data. And usually doing these reciprocally for the two predictors.

To work through this example, we will use the (apparent) trade-off between fat content and lactose content in mammal milk. We used this example in *Quantitative Methods 1* to show how multiple regression is actually working.

Load the data in `Milk.xlsx`, select the columns `kcal.per.g`, `perc.fat`, `perc.lactose`, rename them to `Milk_energy`, `Fat`, and `Lactose`. We will predict the first by the additive effects of the latter two.

There are some missing values in the data, so drop any rows with NA. These are comparative data for different species of primates, but we will ignore those relationships for this analysis.

```{r}

```

Make two plots, one where energy is predicted by fat and the other by lactose.

```{r}

```

You will see that they vary inversely. As fat goes up, lactose goes down. Because there is a finite percentage (100%) of what milk can be made of. As one goes up the other goes down. The third component, protein (mostly casein), makes up the last component. We are ignoring protein.

If you check the correlation between fat and lactose, you will see it's large ($r \approx$ -0.94). In a frequentist regression, you might be worried about multicollinearity in this case.

```{r}

```

### Model specification

```{=tex}
\begin{align}
  \mathrm{Milk\_energy} & \sim Normal(\mu, \sigma) \\
  \mu & = b0 + b1 \mathrm{Fat} + b2 \mathrm{Lactose} \\
\end{align}
```
### Prior specification and prior predictive check

```{r}

```

### Final model specification

```{=tex}
\begin{align}
  \mathrm{Milk\_energy} & \sim Normal(\mu, \sigma) \\
  \mu & = b0 + b1 \mathrm{Fat} + b2 \mathrm{Lactose} \\
\end{align}
```
### Sampling

```{r}

```

### Diagnostics

```{r}

```

### Posterior predictive simulation

Use `pp_check()` to make a density plot of the observed data superimposed on draws from the posterior.

```{r}

```

To visualize the effect of the two continuous predictors, we'll have to get creative. Here are the steps:

-   Make a grid of observations for prediction. Make a sequence of 200 values between 3 and 56 for `Fat`. Specify only three values for `Lactose`: 30, 50, and 70. Each value of `Fat` will be associated with three levels of `Lactose`.
-   Generate the posterior predictive distributions use `posterior_epred()` and the new data you just created.
-   Calculate the median and 89% HDI intervals using `mutate()` like we did in the lecture slides.

```{r}

```

You should have a `tibble` of 600 x 5 columns, with columns for `Fat`, `Lactose`, `Q50`, `Q5.5`, and `Q94.5`.

Make a ribbon plot of the 89% interval, add a line for the median, and facet by `Lactose` in 3 columns. You should be able to see what the model predicts for milk energy as a function of fat at the three levels of lactose.

It will take some staring at this plot to make sense of it. Pay particular attention to the places where the model is pretty sure (narrow bands) or unsure (wide bands).

If you make a composite plot with the pair of scatterplots from the first chunk in this example in one row and this new plot in row 2, it might help to make sense of the output.

```{r}

```

### Summarizing the posterior

Summarize the posterior however you think is appropriate.

```{r}

```
