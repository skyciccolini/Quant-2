---
title: "Problem Set 4"
author:
  - Skylar Ciccolini
format: 
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    toc-title: Contents
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
library(readxl)
library(furrr)
library(purrr)
library(cowplot)
theme_set(theme_cowplot())

# Required files for this problem set:
#   - Stickleback_Salinity.csv
#   - Streams.xlsx

```


# Sticklebacks Revisited

Reopen problem set 2 and revisit the Sticklebacks randomization test you performed. In that activity, you already calculated the expected false positive rate from your empirical null based on 10,000 randomizations for a single threshold median difference value. Let's use the method we discussed in lecture to calculate the 5% FWER threshold. This threshold is what would typically be used for a test like this when using post-hoc tests (e.g. Tukey's). For each of your 10,000 randomizations, keep the maximum absolute median difference across the three pairwise median differences you calculated and make a histogram. Then use the `quantile()` function to find the 5% FWER threshold (i.e., the median difference threshold where you only find any greater difference randomly 5% of the time). Note you should only include your shuffled data here, not your observed set in row 1.   

```{r}
#| echo: true

# read in stick
library(abdData)

SP <- abdData::SticklebackPlates

niter <- 10000
output_r <- tibble("MM_Mm_median" = rep(NA, niter),
                   "MM_mm_median" = rep(NA, niter),
                   "Mm_mm_median" = rep(NA, niter))

for (ii in 1:niter) {
  SP.s <- SP
  
  SP.s$genotype <- sample(SP.s$genotype)
  
  rand.st <- SP.s |> 
    group_by(genotype) |>
    summarize(Median = median(plates))
  
  output_r$MM_Mm_median[ii] <-  rand.st$Median[rand.st$genotype=="MM"] -
    rand.st$Median[rand.st$genotype=="Mm"]
  output_r$MM_mm_median[ii] <-  rand.st$Median[rand.st$genotype=="MM"] -
    rand.st$Median[rand.st$genotype=="mm"]
  output_r$Mm_mm_median[ii] <-  rand.st$Median[rand.st$genotype=="Mm"] -
    rand.st$Median[rand.st$genotype=="mm"]
}

out_plot <- output_r |>
  pivot_longer(everything(), names_to = "Pair", 
                       values_to = "Difference")

out_plot |>
  ggplot(aes(Difference)) +
  geom_histogram(fill = "grey75", bins = 50)


# FWER
quantile(out_plot$Difference, prob = 0.95) ### pretty sure this is wrong

```

Using false discovery methods using empirical distributions will typically be applicable when you are performing many more than just 3 tests. Take a second to consider what your possible FDR values can be in the Sticklebacks example. Let's now consider a hypothetical gene expression data set measuring the expression of 9,000 genes comparing two salinity treatments in a set of sticklebacks. Read in the `Stickleback_Salinity.csv` file and examine its structure. There is a `Salinity` column denoting the treatment with 10 samples in each. Then, each column represents the expression values for one gene. 


```{r}
#| echo: true

stick <- read_csv("raw_data/Stickleback_Salinity.csv")

glimpse(stick)

head(stick)

```

Use a single `lm()` to predict the matrix of gene expression measures from the salinity treatment for your observed data. Use the function provided below to extract the p-values for your 9,000 y's and save this as an object. Note your y's will need to be a matrix for this to work and don't forget to not include the treatment column. Hint: tibbles keep their dimensions when you use `[]` to select a single column so you might want to use the `$` method to select your treatment variable. Or you can use `drop = TRUE`. Make a histogram of your observed *P*-values.

```{r}
#| echo: true

# linear model
fm <- lm(as.matrix(stick[ , 2:9001]) ~ Salinity, data = stick)

getP <- function(fm) {
  sum.set <- summary(fm)
  p.set <- lapply(sum.set, function(x) x[['coefficients']][2, 4])
  return(unlist(p.set))
}

obsP <- getP(fm)


ggplot(tibble(obsP), aes(obsP)) +
  geom_histogram(fill = "grey75") +
  xlab("Observed P-values") +
  geom_vline(xintercept = 0.05, color = "firebrick4", linewidth = 2)
```



Write a function set up for using `future_map()` that will do one randomization and return the number of false positives for a range of thresholds. 

```{r}
#| echo: true


FDR <- function(ii, stick) {
  mod.s <- lm(as.matrix(stick[sample(1:nrow(stick)) , 2:9001]) ~ Salinity, data = stick)

sampPs <- getP(mods.s)

ctPs[ii, ] <- sapply(thresh, function(x) sum(sampPs < x))
}


mods.s <- lm(as.matrix(stick[sample(1:nrow(stick)) , 2:9001]) ~ Salinity, data = stick)
sampPs <- getP(mods.s)




```

Now use `future_map()` to do 1,000 permutations, keeping the number of false positives for your range of thresholds. You should do fewer permuations until you are sure everything is working ok. It will take a few minutes to run 1,000, even in parallel. You will end up with a list with each element containing the set of false positives for your thresholds for one permutation.

```{r}
#| echo: true

niter <- 10
thresh <- c(seq(1e-16, 0.001, length.out = 60),
         seq(0.002,0.05, length.out = 40))
ctPs <- matrix(NA, niter, length(thresh))


outs <- future_map(.x = seq_len(niter),
           .f = FDR,
           stick = stick,
           .options = furrr_options(seed = TRUE))


### run this with 1000 iters
```

Use `as.data.frame()` to turn the list output into a data.frame with the permutations in columns and thresholds in rows. The column names will be crazy but that is ok. You can rename them with `colnames()` if you want. Get the average number of false positives at each threshold. Then calculate the number of positives in your observed data at each threshold. Finally calculate the FDR at each threshold and plot FDR vs threshold.

```{r}
#| echo: true

# create df
outdf <- as.data.frame(outs)

# average false positive in each thresh
FPs <- mean(ctPs)

# number of positives in obs
APs <- sum(obsPs < d.th)

# find FDR at each thresh
FPs/APs

# plot FDR vs thresh

fp.out <- tibble(thresholds = thresh,
                 False = apply(ctPs, 2, mean),
                 All = sapply(thresh, function(x) sum(obsPs < x)))

fp.out$FDR <- fp.out$False / fp.out$All

p2 <- fp.out |>
  ggplot(aes(thresholds, FDR)) +
  geom_point() +
  geom_hline(yintercept = 0.05, color = "firebrick4", linewidth = 1.5) +
  labs(x = "Threshold")

### run this

```

What *P*-value threshold would you recommend for this study?

> ###

## Sampling strategies

Imagine you are planning an experiment looking at the response of a plant population to herbivory by aphids. You plan to measure salicylic acid (SA) in plants in two treatments, either with or without aphids. There is known technical variation in the assay for salicylic acid so you want to plan how many technical replicates (sampling from a plant and assaying aliquots from the same sample multiple times) versus biological samples (assaying additional plants) you should do. 

Let's walk through a single simulated dataset to consider how we might simulate date to assess power. As you go through these, print and check things are behaving as expected often. You may want to set some extreme values to start so you can visually detect that simulated values are what you expect them to be (e.g., a really big difference in groups).

1. Create the following as objects (you choose some values to start with):
    - the number of biological samples in each group
    - the difference in means between the aphid group and control
    - the standard deviation in the aphid group (how much variation in SA is there among different plants exposed to aphids)
    - the standard deviation in the control group (how much variation in SA is there among different plants not exposed to aphids)
2. Make a tibble holding the treatment ids and SA values for your simulated data. Use a normal distribution to create your SA values using the parameters you've defined. Use 0 as the mean for the control and 0 + your difference between groups for the aphid treatment. Because we are interested in the relative differences in groups, we don't need to use absolute numbers on the scale you measure SA on.   

```{r}
#| echo: true

bio <- 50
mu_diff <- 10
sd_aphid <- 3
sd_control <- 1

Group <- c("aphid", "control")


sim <- tibble(
  ID = sample(Group, size = bio, replace = TRUE),
  SA = case_when(sim$ID == "control" ~ rnorm(bio, 0, sd_control),
                  sim$ID == "aphid" ~ rnorm(bio, (0 + mu_diff), sd_aphid))
)


nsims <- 1e4
alpha <- 0.05
ns <- c(5, 10, 25, 50, 100, 200, 400)
b_null <- 1/3
b_devs <- seq(-0.2, 0.2, length.out = 100)

# All combinations of ns and b_devs
pwr_reg <- crossing(ns, b_devs)
names(pwr_reg) <- c("n", "b_dev")
pwr_reg$Power <- NA

```

Now we can think about the technical variation in the test. Let's use the values you just simulated as the true SA values for each of your biological samples. Then we can generate a number of technical replicates using these values. 

3. Create the following as objects (you choose some values to start with):
    - the number of technical replicates
    - the standard deviation of the technical replicates (how much variation is there for repeated measurements of the same sample?)
4. Create a new tibble with a column with `PlantID`, `Treatment`, and your SA values. Use the normal distribution to generate a set of values for each of your plants using each of the values you made above as the mean for a given plant and the variation for technical replication you have set. 

You can pass a vector of means and standard deviations to `rnorm()`. This can be handy for these kinds of simulations. Look at the code below to see how this works. Note, that it won't work to pass it a vector of `n`s. You should tell it how many samples you need and then provide vectors of that same length of means and sds. That might require repeating values if you need more than one sample for a combination. 

```{r}
#| echo: true

n_tech <- 5
sd_tech <- 2

tib <- tibble(
  PlantID = rep(1:bio, each = n_tech),
  Treatment = rep(sample(Group, size = bio, replace = TRUE), each = n_tech)) |> 
  mutate(
  SA = case_when(Treatment == "control" ~ rnorm(n(), 0, sd_control),
                  Treatment == "aphid" ~ rnorm(n(), (0 + mu_diff), sd_aphid)),
  SA_sim = case_when(Treatment == "control" ~ rnorm(n(), 0, sd_tech),
                  Treatment == "aphid" ~ rnorm(n(), (0 + mu_diff), sd_tech))
)


num <- 10
mu <- 0
ss <- 1

rnorm(num, mu, ss)

mus <- c(rep(0, num/2), rep(20, num/2))
sss <- rep(1, num)

rnorm(num, mus, sss)

```


```{r}
#| echo: true



```

5. Now fit a mixed model predicting SA from treatment with your plant ids as a grouping variable and extract the *P*-value for treatment . 

```{r}
#| echo: false

library(lmer)

mixmod <- lmer(SA_sim ~ Treatment + (1 + Treatment| PlantID),
               data = tib)

obsp <- summary(mixmod)$coefficients["Treatmentcontrol"] ### not working

```

Now you have all the tools you need to set up a simulation where you could explore:

1. the number of biological replicates
2. the number of technical replicates
3. the variation within treatment groups
4. the variation within technical replicate groups
5. the difference between the treatment and control groups

Focus on exploring the number of biological vs technical replicates an how that changes with the variation within technical replicate groups. Choose a single parameter value for the variation within treatment groups (e.g., 1) and the difference between the treatment and control groups. Choose a total number of samples (e.g., 100) and test a range of values allocating these to biological samples or technical replicates. Then do a simulation with a range of values for the variation in technical replicate groups and visualize your results. 

```{r}
#| echo: true

### do this


```


## Cross-validation

In QMLS1, problem set 10, you analyzed data from Snyder et al. (2015), in which they collected data on the response of water temperature to air temperature to better understand the time scale of how changing air temperature affects water temperature (e.g., as a result of global climate change). The data they collected are in the file `Streams.xlsx`, the raw data file provided by the authors on Dryad.

One goal is to be able to predict water temperature only using air temperature (since air temperature is more efficiently measured at large scale).

Run the chunk below to load the data and set it up as we did in QMLS1. You may need to change the path depending on where you store the data. 

```{r}
#| echo: true

Temp_Data <- read_excel("raw_data/Streams.xlsx",
                        sheet = "TemperatureData")

Site_Data <- read_excel("raw_data/Streams.xlsx", sheet = "SiteData")

MM <- left_join(Temp_Data, Site_Data)

MM <- MM |>
  mutate(Site = factor(Site),
         Date = factor(Date),
         Stream_Name = factor(Stream_Name),
         Day = as.numeric(Date))

```

In QMLS1, we fit two models to these data. In both models we want to model water temperature by predicted air temperature. The difference between the models will be in the random effects. Let's fit these same two using `lme()` here to start. 

1. Fit a multilevel model where the intercept for site nested in stream is random (`~ 1 | ...`). This model allows each `Site` to have it's own intercept, which is nested within `Stream_Name`. But this value does not change over time.
2. Fit a multilevel model where the intercept for site nested in stream is random and day is also included as a continuous random predictor (`~ Day | ...`).

```{r}
#| echo: true

library(lme4)

mod1 <- lmer(WaterTemp ~ AirTempPredicted + (1 | Stream_Name) + (1 | Stream_Name:Site), 
           data = MM)

mod2 <- lmer(WaterTemp ~ AirTempPredicted + Day + (1 | Stream_Name) + (1 | Stream_Name:Site), 
           data = MM)

```

In QMLS1, we compared models using AIC. Let's take a cross-validation approach here. The approach to leaving observations out is not straight-forward here, given the observations are not independent. Let's take the approach where we leave one observation out at a time. This leave-one-out approach is not ideal, because sites are nested within streams, but if we leave out an entire site or stream, then we can't do prediction in the usual way (i.e., with `predict()`).

Use LOOCV to compare both models above, using `predict()` to get predicted values for the observation you left out. Compare this set of values to the observed values to get the mean absolute error and keep this value for each stream. There are 3,744 observations, so you might consider using `future_map()`.

```{r}
#| echo: true

MM <- MM |> 
  mutate(k = sample(rep(1:3744)))

CV_fun_1 <- function(k_fold, MM) {
  MMk <- MM |> filter(k != k_fold)
  lmk <- lmer(WaterTemp ~ AirTempPredicted + (1 | Stream_Name) + (1 | Stream_Name:Site), 
           data = MMk)
  
  MM |> filter(k == k_fold) |> 
    mutate(y_hat = predict(lmk, newdata = MM |> filter(k == k_fold)),
           Error = y - y_hat,
           Squared_error = Error ^ 2,
           Absolute_error = abs(Error))
}


CV_mod1 <- future_map(.x = 1:3744,
                    .f = CV_fun_1,
                    MM = MM) |> 
  list_rbind()

CV_fun_2 <- function(k_fold, MM) {
  MMk <- MM |> filter(k != k_fold)
  lmk <- lmer(WaterTemp ~ AirTempPredicted + Day + (1 | Stream_Name) + (1 | Stream_Name:Site), 
           data = MMk)
  
  MM |> filter(k == k_fold) |> 
    mutate(y_hat = predict(lmk, newdata = MM |> filter(k == k_fold)),
           Error = y - y_hat,
           Squared_error = Error ^ 2,
           Absolute_error = abs(Error))
}

CV_mod2 <- future_map(.x = 1:3744,
                    .f = CV_fun_2,
                    MM = MM) |> 
  list_rbind()


```


Which model would you conclude is preferable?

> ###




