---
title: "Problem Set 3"
author:
  - Your Name Here
format: 
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    toc-title: Contents
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
library(furrr)
library(purrr)
library(nlme)
library(lmtest)
library(parallel)
library(parallelly)
library(cowplot)
theme_set(theme_cowplot())
set.seed(7447023)

# Required files for this problem set:
#   - NeandertalBrainSize.csv
#   - prairie.csv

```


## Neandertal Brains

In problem set 10 in QMLS1, you used a dataset on modern humans and Neanderthals to ask about differences in brain size when adjusted for body size.^[Ruff, C.B., E. Trinkaus, and T.W. Holliday. 1997. Body mass and encephalization in Pleistocene *Homo*. *Nature* 387: 173-176.]
  
The file `NeandertalBrainSize.csv` contains data on estimated log body mass, log brain size, and `Species`. Load the file, and convert `Species` to a factor.

```{r}
brain <- read_csv("raw_data/NeandertalBrainSize.csv") |> 
  mutate(Species_fct = as.factor(Species))

```

One of the models we fit was: Brain size modeled by body mass and species (additive model only without the mass X species interaction). Fit this model and perform a randomization test for each of your predictors performing 10,000 iterations. 

```{r}
# randomize two predictors
brain.shuffle <- brain[sample(1:nrow(brain)),c("ln_Mass","Species_fct")]

# add response to shuffled
brain.shuffle$ln_Brain <- brain$ln_Brain

# model
brain_lm <- lm(ln_Brain ~ ln_Mass + Species_fct, data = brain.shuffle)

# pull out p vals
obs <- broom::tidy(summary(brain_lm))[2:3, c(1, 5)]

obs

# predict from model
brain.shuffle <- brain.shuffle %>% mutate(pred = predict(brain_lm))

ggplot(brain.shuffle, aes(x = ln_Mass, y = ln_Brain, color = Species_fct)) +
  geom_point(size = 4) +
  geom_line(aes(x = ln_Mass, y = pred, color = Species_fct), lwd = 2) +
  theme(legend.justification = c(0, 1), legend.position = c(0.05, 1)) +
  labs(x = "ln Body Mass (g)", y = "ln Brain Mass") +
  scale_color_viridis_d()





# randomization
niter <- 10000
brain.shuffle <- brain
out.ps <- tibble("term" = rep(NA, niter * 2), 
                 "p.value" = rep(NA, niter * 2))
out.ps[1:2, ] <- obs
counter <- 3

for (ii in 2:niter) {
  brain.shuffle <- brain[sample(1:nrow(brain)),c("ln_Mass","Species_fct")]

brain.shuffle$ln_Brain <- brain$ln_Brain

brain_lm.s <- lm(ln_Brain ~ ln_Mass + Species_fct, data = brain.shuffle)

out.ps[counter:(counter + 1), ] <- broom::tidy(summary(brain_lm.s))[2:3, c(1, 5)]
  counter <- counter + 2

}

```


## Chi-Squared Test

The Chi-squared test is known to have biases when there are relatively few observations for one or more categories. Look at the help for `chisq.test` and specifically read about the `simulate.p.value` option. Using the `simulate.p.value` option to use a Monte Carlo simulation is recommended when sample size is low.

Let's explore this for two back-cross experiments designed to ask whether a trait is determined by a single gene with dominance. Garland et al. (2008) found that a "mini-muscle" phenotype was present in two lines in an experiment selecting for high wheel running in mice. They then performed a back-cross design (cross mini-muscle mice to wild-type and then cross the offspring to mini-muscle mice again). If the mini muscle phenotype is determined by a single gene and that gene is recessive, you expect a 1:1 ratio in the offspring of the backcross. [Hannon et. al (2008)](https://academic.oup.com/jhered/article/99/4/349/2187936) found 201 offspring with the mini muscle phenotype and 203 without. Feel free to work out the Punnett squares for yourself.

Perform a chi-squared test with and without `simulate.p.value = TRUE`.

```{r}

mice <- as.table(c(201, 203))
dimnames(mice) <- list(pheno = c("muscle", "wild"))

(chi <- chisq.test(mice, simulate.p.value = TRUE))

(chi <- chisq.test(mice, simulate.p.value = FALSE))

```

There is not a way to randomize these counts by shuffling, because, if you simply shuffle the genotypes, you will end up with the same counts (the Chi-squared test uses aggregated counts only). Instead, the Monte Carlo simulation used here produces a null expectation based on the total sample size and the null hypothesis. Here, the test is for a 1:1 ratio or equal proportions. Simulate a set of two counts with a 1:1 expectation. Then repeat this 2,000 times and calculate a *P*-value based on this simulation. 

```{r}
obs <- c(201, 208)
obs_diffs <- abs(obs[1]-obs[2])
n <- 409
niter <- 2000
sim_diffs <- as.vector(length(niter))

for (ii in 1:niter) {
  # simulate number of muscle mice
    mus <- rbinom(n = 1, size = n, prob = 0.5)

  # difference between groups
    sim_diffs[ii] <- abs(mus - (n -mus))
}


mean(sim_diffs >= obs_diffs)

```

Compare your results to the output of the `chisq.test()` function with simulation you used above.

> My empirical p value is about 0.2 smaller than the chi square p value.

Now consider another similar back-cross experiment looking at a different phenotype, a white head fur blaze, with a much smaller sample size. Here, 12 individuals had the white blaze and 9 did not. Repeat all the steps above for this example.  


```{r}
# 12 with white, 9 not
mice <- as.table(c(12, 9))
dimnames(mice) <- list(pheno = c("w", "n"))

(chi <- chisq.test(mice, simulate.p.value = TRUE))

(chi <- chisq.test(mice, simulate.p.value = FALSE))

obs <- c(12, 9)
obs_diffs <- abs(obs[1]-obs[2])
n <- 21
niter <- 2000

for (ii in 1:niter) {
  # simulate number of muscle mice
    mus <- rbinom(n = 1, size = n, prob = 0.5)

  # difference between groups
    sim_diffs[ii] <- abs(mus - (n -mus))
}


mean(sim_diffs >= obs_diffs)

```

Consider how the "null hypothesis" is being used statistically here. How well do you think this test supports a conclusion of a 1:1 ratio in the second example and how certain should you be about that conclusion? Do you have any ideas for a different simulation that might be more appropriate for making a conclusion about the inheritance mechanism of the white blaze?

> I don't feel very certain that a 1:1 ratio is present in the second example. Maybe I could increase the number of iterations?


## Restoration Treatments in Prairies in a Block Design

Block designs are used often to help isolate the effects of treatments. Consider a block design where there are three plots within each block in space in a prairie restoration. These three plots are a control (unmanipulated), burned annually in fall, and mowed every spring. Within these blocks, there are several quadrants where species richness was measured. Load in the prairie.csv file and examine the structure of the data, plotting, etc.

```{r}

prairie <- read_csv("raw_data/prairie.csv")

head(prairie, 10)

ggplot(data = prairie) +
  geom_point(aes(x = Block, y = Richness, color = Treatment))
```

Fit a multilevel model using `lme()` with Block as a random effect. To assess the significance of the treatment effect overall, fit the model with `method = "ML"` to use maximum likelihood. Then fit a model without treatment and use `lrtest()` in the `lmtest` package to perform a likelihood ratio test comparing the models with and without treatment. Unit 10 in QMLS 1 covered likelihood ratio tests, and PS 10 has examples.

```{r}

mod <- lme(Richness ~ Treatment, random = ~ 1 | Block, 
           data = prairie, method = "ML")

mod_noml <- lme(Richness ~ 1, random = ~ 1 | Block,
                data = prairie, method = "ML")

lrt <- lrtest(mod, mod_noml)

obs <- as.numeric(lrt$`Chisq`[2])

```

Here, observations are not exchangeable between experimental units. Thus, we need to randomize treatment within each block. Perform a small set of just 100 shuffles within blocks and perform the same likelihood ratio test using the chi-squared statistic as your statistic of interest. Estimate the time it takes to run this set. Then calculate how long it would take to run 10,000 permutations.

```{r}
# shuffle once

blk <- unique(prairie$Block)

prairie.s <- prairie |> group_by(Block) |>
  mutate(Treatment.s = sample(Treatment))

# initial chi
obs <- obs

# shuffle more than once
niter <- 100
output <- tibble("chi" = rep(NA, niter))
output$chi[1] <- obs

for (ii in 2:niter) {
  prairie.s <- prairie |> group_by(Block) |>
  mutate(Treatment.s = sample(Treatment))
  
 mod <- lme(Richness ~ Treatment.s, random = ~ 1 | Block, 
           data = prairie.s, method = "ML")

mod_noml <- lme(Richness ~ 1, random = ~ 1 | Block,
                data = prairie.s, method = "ML")

lrt <- lrtest(mod, mod_noml)

  output$chi[ii] <- as.numeric(lrt$`Chisq`[2])
}

```

This would take a bit of time (estimated at about 200 seconds on this machine) so let's use the `future_map_dbl()` approach. Convert your code into a function that will accept an iteration number and the data and will return your test statistic. Test the function a couple times to ensure you get the expected result.   

```{r}
niter <- 100

chi_fcn <- function(ii, prairie) {
  prairie.s <- prairie |> group_by(Block) |>
  mutate(Treatment.s = sample(Treatment))
  
 mod <- lme(Richness ~ Treatment.s, random = ~ 1 | Block, 
           data = prairie.s, method = "ML")

mod_noml <- lme(Richness ~ 1, random = ~ 1 | Block,
                data = prairie.s, method = "ML")

lrt <- lrtest(mod, mod_noml)

  d <- as.numeric(lrt$`Chisq`[2])
  return(d)
}

furr_out <- future_map_dbl(.x = seq_len(niter),
                      .f = chi_fcn,
                      prairie = prairie,
                      .options = furrr_options(seed = TRUE))


```

Figure out how many cores your local computer has using `availableCores()` and use `plan()` to set up a multisession. Make sure to omit 1 core for the OS to keep running. Then use `future_map_dbl` with your function to perform 10,000 permutations. Feel free to time this chunk of code to see what the realized speed increase is.

```{r}
# find number of cores
(ncores <- parallelly::availableCores(omit = 1))

plan(multisession, workers = ncores)

niter <- 10000
furr_out <- future_map_dbl(.x = seq_len(niter),
                      .f = chi_fcn,
                      prairie = prairie,
                      .options = furrr_options(seed = TRUE))

furr_tib <- as_tibble(furr_out)

```

Make a histogram of your chi-squared test statistic and add a line for your observed. Calculate an empirical p-value. 

```{r}
ggplot(data = furr_tib) +
  geom_histogram(aes(value)) +
  geom_vline(aes(xintercept = obs), color = "red")

# calculate empirical p value
emp.ps <- apply(furr_tib, 2, function(x) sum(abs(x) >= abs(x[1]))/length(x))

```




